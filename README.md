<div style="display: flex; justify-content: space-around; align-items: center;">
  <img src="assets/images/hbert_arch.JPG" alt="Image 1" style="width: 100%; margin: 10px;">
</div>

## ğŸ“œ paper-hbert-sarcasm-detection
#### ğŸ“Œ Summary 
Implementation of the paper : 
**["A Novel Hierarchical BERT Architecture for Sarcasm Detection"](https://aclanthology.org/2020.figlang-1.14.pdf)** using **Python**, **Pandas**, **NumPy**, **Kaggle** and **TensorFlow**.

#### ğŸ§  Overview
This project implements a Hierarchical BERT (H-BERT) architecture for sarcasm detection in text, inspired by the paper **"A Novel Hierarchical BERT Architecture for Sarcasm Detection".** This model is a combination of LSTM & CNN with the BERT embeddings. Unlike traditional sentiment analysis tasks, sarcasm detection often requires understanding multi-sentence context and subtle cues. This hierarchical model addresses that by processing text at both the sentence and context level using a 5 layered architecture that includes (1) sentence encoding, (2) context summarization with Conv1D, (3) a bi-directional LSTM for capturing temporal dependencies, (4) CNN layers for local feature extraction, and a (5) final fully connected layer for classification.

The model is trained on the **"Sarcasm on Reddit"** dataset from Kaggle rather than the dataset used in the original paper. Preprocessing includes data cleaning and dimensionality reduction. Tokenization is done using the **'bert-base-uncased tokenizer'** from Hugging Face. Evaluation is performed using accuracy as the primary metric, and the entire model is implemented in Python using TensorFlow.

#### ğŸ¯ Use Cases 
- Detecting Sarcastic comments or opinions on social media platforms

#### ğŸŸ¢ Project Status
- Current Version: V1.0
- Completed

#### ğŸ“‚ Repository Structure
```
paper-hbert-sarcasm-detection/
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â”œâ”€â”€ .gitignore                  
â”œâ”€â”€ assets/                      
â”‚   â””â”€â”€ images/
â”œâ”€â”€ datasets/                      
â”‚   â””â”€â”€ sarcasm-reddit/
|   â”‚   â””â”€â”€ sarcasm_trained_balanced.csv
â””â”€â”€ notebooks/               
    â””â”€â”€ sarcasm-analysis.ipynb            
```

### âœ¨ Features
- âœ… Custom `Hierarchical-BERT` model class
- âœ… Preprocessed Data
- âœ… Sentence tokenization and padding
- âœ… Evaluation metrics: Accuracy 

ğŸ› ï¸ In progress:
- Modification of H-bert Architecutre 
- Introducing Precision, Recall, F1-score

<!--
### ğŸ¥ Demo
 <a href="https://www.youtube.com/shorts/wexIv6X45eE?feature=share" target="_blank">
  <img src="assets/images/2_2.JPG" alt="YouTube Video" width="390" height="270">
</a> 
-->

### ğŸš€ Getting Started
#### ğŸ“š Knowledge & Skills Required 
- Python programming
- ML/DL fundamentals, Transformers, Hugging Face Hub
- Neural Network Arhitecutres (CNN, LSTM, RNN, BERT)

#### ğŸ’» Software Requirements
- IDE (VS Code) or jupyter notebook or google colab
- **Best run on Kaggle using GPU P100**
  
#### ğŸ›¡ï¸ Tech Stack
- Language: python
- NLP/ML: huggingface_hub, sklearn, pandas, numpy
- Deep Learning: pytorch, CNN, LSTM, transformers-models (bert-base-uncased) 
- Visualization: matplotlib

#### ğŸ” Modules Breakdown
<b> ğŸ“¥ (1) Data-Preprocessing :</b> wh 
- Using a different Dataset than what was used on paper
- Loading [Sarcasm on Reddit](https://www.kaggle.com/datasets/danofer/sarcasm?select=train-balanced-sarcasm.csv) dataset from kaggle 
- Dimention Reduced, Data Cleaned

<b> ğŸ­ (2) Tokenization :</b> basi
- Used **'bert-base-uncased'** model from hugging face for tokenization
- Tokenized "comment" column

<b> ğŸ¤– (3) Heirarchial-BERT :</b> 
- The proposed H-BERT Architecture proposed in the paper and implemented in code is explained beiefly. The model consists of **5 layers** :-
  1. **(L-1) Sentence Encoding Layer**  
     - Encode input data into fixed sized vectors
  2. **(L-2) Context Summarization Layer**  
     - Convert the indvidual sequence-embeddings from layer-1 to common single-vect
     - **Conv1d sentence-summarizer layer** : Paper used Conv2d as their data 1.3B but we use only 30k so we use Conv1d
  3. **(L-3) LSTM - Context Encoder Layer**  
     - Implement a bi-direc LSTM to capture temporal dependencies from summ-sent layer-2
     - Bi-direc LSTM process data in both forward & backward direc, makes capturing v. easy
  4. **(L-4) CNN Layer**  
     - Extracts local features from encoded context vectors of layer-3
     - Try to emphasis significant features relavant to model and 
     - Give less attn to irrelavant features
     - **Kernel-layer**  : We use Conv1d kernels instead of Conv2d, also called cnn-filters
     - **Pooling-layer** : Use max pooling to extract imp features
  5. **(L-5) FFN / Fully Connected dense-net Layer**  
     - Proecess the model to give a final output 
     - Maps features to final predictions.

- The DataFlow:- 
(L-1) output --> (L-2) output --> (L-3) output --> (L-4) output --> (L-5) output --> predictions (0 or 1)


##### ğŸ“Š Evaluation
- Using 'Accuracy' Meterics to evaluate model performance
- Future work : precision , recall , f1

#### âš™ï¸ Installation
```
git clone https://github.com/pointer2Alvee/paper-hbert-sarcasm-detection.git
cd hbert-sarcasm-detection

# Recommended: Use a virtual environment
pip install -r requirements.txt
```

##### ğŸ–‡ï¸ requirements.txt (core packages):
```
transformers==4.44.0
huggingface_hub==0.24.5
pandas
numpy
tensorflow
matplotlib
```

##### ğŸ’» Running the App Locally
1. Open Repo in VS code / Kaggle (recommended)
2. Run Command
3. See accuracy

#### ğŸ“– Usage
- Open VS Code / kaggle

### ğŸ§ª Sample Topics Implemented
- âœ… BERT model 
- âœ… LSTM, CNN, CONVOLUTION, POOLING
  
- â³ Upcoming  : Modification of Hbert

### ğŸ§­ Roadmap
- [x] Paper Implementation of Hbert 


### ğŸ¤ Contributing
Contributions are welcomed!
1. Fork the repo. 
2. Create a branch: ```git checkout -b feature/YourFeature```
3. Commit changes: ```git commit -m 'Add some feature'```
4. Push to branch: ```git push origin feature/YourFeature```
5. Open a Pull Request.

### ğŸ“œLicense
Distributed under the MIT License. See LICENSE.txt for more information.

### ğŸ™Acknowledgements
- Special thanks to the open-source community / youtube for tools and resources.
